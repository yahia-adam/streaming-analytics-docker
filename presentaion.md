
# ğŸ“ PrÃ©sentation de Projet : **Streaming & Visualisation avec Docker**

---

## ğŸ” Contexte

* **Ã‰cole** : ESGI
* **Niveau** : 4e annÃ©e
* **FiliÃ¨re** : Intelligence Artificielle & Big Data
* **Cours** : Conteneurisation logicielle (Docker)
* **Ã‰quipe** :

  * TRAN DUC Bao Nguyen
  * BADIABO Divin
  * ELOY ThÃ©o
  * YAHIA ABDCHAFEE Adam

---

## ğŸ¯ Objectif du projet

> Concevoir et dÃ©ployer une **chaÃ®ne complÃ¨te de traitement de donnÃ©es en streaming** conteneurisÃ©e avec **Docker Compose**, intÃ©grant :

* Un **Producer** Spark qui lit les fichiers localement
* Un **Kafka broker** pour la transmission des donnÃ©es
* Un **Consumer** Spark qui consomme les donnÃ©es Kafka et les stocke en base de donnÃ©es
* Une **base PostgreSQL**
* Une application **Streamlit** pour la visualisation web

Le focus est mis sur :

* La **conteneurisation complÃ¨te** des services
* La **gestion rÃ©seau sÃ©curisÃ©e**
* Lâ€™**automatisation** avec Docker Compose
* Le **bon usage des volumes, images, variables dâ€™environnement, sÃ©curitÃ© rÃ©seau**

---

## ğŸ—ï¸ Architecture technique

![Architecture du projet](attachment:/mnt/data/bd03109a-2a0e-4d7f-85f5-280c9e3b8e43.png)

### ğŸ§± Composants principaux

| Composant    | Fonction                                                           |
| ------------ | ------------------------------------------------------------------ |
| `Producer`   | Lit les fichiers Yelp localement et envoie les donnÃ©es dans Kafka  |
| `Kafka`      | Assure la distribution des messages (topic `yelp-topic-review`)    |
| `Consumer`   | RÃ©cupÃ¨re les donnÃ©es Kafka et les stocke dans PostgreSQL via Spark |
| `PostgreSQL` | Stocke les rÃ©sultats traitÃ©s pour visualisation                    |
| `Streamlit`  | Interface web de visualisation                                     |
| `Zookeeper`  | Coordination du broker Kafka                                       |

---

## âš™ï¸ Infrastructure Docker

### ğŸ”¹ Orchestration avec Docker Compose

Tous les services sont orchestrÃ©s dans un fichier `docker-compose.yml`, incluant :

* **Construction des images custom (Producer / Consumer / Streamlit)**
* **Utilisation de volumes** : pour persistance (`postgres_data`) et partage de dataset
* **Utilisation de `.env`** pour sÃ©curiser les accÃ¨s Ã  PostgreSQL
* **Utilisation de `depends_on`** pour gÃ©rer les ordres de dÃ©marrage

### ğŸ”¹ RÃ©seaux Docker personnalisÃ©s

| RÃ©seau       | Type   | Composants inclus         | But                                     |
| ------------ | ------ | ------------------------- | --------------------------------------- |
| `engnetwork` | PrivÃ©  | Kafka, Producer, Consumer | Communication interne sÃ©curisÃ©e des ETL |
| `dbNetwork`  | PrivÃ©  | Consumer, PostgreSQL      | Connexion base de donnÃ©es sÃ©curisÃ©e     |
| `webNetwork` | Public | Streamlit, PostgreSQL     | AccÃ¨s externe Ã  l'application web       |

> âš ï¸ Les rÃ©seaux sont **isolÃ©s**, seuls les services qui doivent communiquer entre eux sont sur les mÃªmes rÃ©seaux.

---

## ğŸ³ DÃ©tails des Dockerfiles

### ğŸ”¸ Spark (Producer & Consumer)

* **BasÃ© sur `openjdk:11`**
* TÃ©lÃ©chargement et configuration de **Spark 3.5.0**
* Installation du `.jar` depuis les **GitHub Releases**
* Configuration JVM pour performance
* Commande de lancement : `spark-submit` avec dÃ©pendances Kafka + PostgreSQL

### ğŸ”¸ Streamlit

* **BasÃ© sur `python:3.10`**
* Installation des dÃ©pendances via `requirements.txt`
* Exposition sur le port `8501`
* Code Python contenu dans `src/Welcome.py`

---

## ğŸ§ª DÃ©monstration & Jeu de donnÃ©es

* **Dataset** : portion de 5000 lignes issues de Yelp
* **Fichiers** : montÃ©s via volume Docker (`./yelp_dataset`)
* **Streamlit** : expose un dashboard interactif (mapping port `8501:8501`)
* **Spark UI** : exposÃ© via port `4040` pour le Producer, `4041` pour le Consumer

---

## ğŸ“¦ Gestion des images Docker

* Les images sont **construites localement** puis **poussÃ©es sur Docker Hub**
* Utilisation possible des images via Docker Compose en changeant le champ `image:` au lieu de `build:`

---

## âœ… Bonnes pratiques respectÃ©es

* ğŸ” **SÃ©paration des rÃ©seaux** public/privÃ©
* ğŸ“ **Utilisation de volumes** pour la persistance
* ğŸ”„ **Redondance limitÃ©e** : Topics Kafka configurÃ©s (3 partitions, 1 rÃ©plica)
* ğŸ“œ **Configuration via `.env`** pour Ã©viter le hardcoding
* ğŸ” **Logs et accÃ¨s UI exposÃ©s** pour debugging et supervision
* ğŸ“¦ **Utilisation de GitHub Releases** pour versionner les `.jar` Spark
* ğŸ“Š **Visualisation claire** des rÃ©sultats pour fin de pipeline

---

## ğŸ”’ SÃ©curitÃ© & performances

| Aspect                | ImplÃ©mentation                                                                 |
| --------------------- | ------------------------------------------------------------------------------ |
| SÃ©curitÃ© rÃ©seau       | RÃ©seaux isolÃ©s, ports exposÃ©s uniquement pour Streamlit & Spark UI             |
| Variables sensibles   | ExternalisÃ©es via `.env`                                                       |
| Ressources maÃ®trisÃ©es | Limites mÃ©moire Spark (`--driver-memory`, `--executor-memory`)                 |
| Volume dataset        | MontÃ© en lecture seule, permissions sÃ©curisÃ©es sur les fichiers intermÃ©diaires |

---

## ğŸš€ Conclusion

Ce projet met en pratique **lâ€™ensemble des compÃ©tences fondamentales en conteneurisation** dans un contexte **Big Data / IA** :

* Mise en Å“uvre dâ€™un **pipeline de streaming complet**
* Isolation des services via rÃ©seaux Docker
* Bonne gestion des volumes, images et configurations
* DÃ©ploiement dâ€™une **application visualisable via le web**
* Application des **bonnes pratiques de sÃ©curitÃ© et de performances**

